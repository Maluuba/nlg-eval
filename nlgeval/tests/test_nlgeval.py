# -*- coding: utf-8 -*-
from __future__ import unicode_literals

import os
import unittest

import nlgeval
from nlgeval import NLGEval


class TestNlgEval(unittest.TestCase):
    def test_compute_metrics_oo(self):
        # Create the object in the test so that it can be garbage collected once the test is done.
        n = NLGEval()

        # Individual Metrics
        scores = n.compute_individual_metrics(ref=["this is a test",
                                                   "this is also a test"],
                                              hyp="this is a good test")
        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)
        self.assertAlmostEqual(0.5108729, scores['Bleu_3'], places=5)
        self.assertAlmostEqual(0.0000903602, scores['Bleu_4'], places=5)
        self.assertAlmostEqual(0.44434387, scores['METEOR'], places=5)
        self.assertAlmostEqual(0.84211, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.4, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.18182, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.0, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.84211, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.68168, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.66667, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.70588, scores['Rouge_SU*'], places=5)
        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)
        self.assertAlmostEqual(0.8375251, scores['SkipThoughtCS'], places=5)
        self.assertAlmostEqual(0.980075, scores['EmbeddingAverageCosineSimilairty'], places=5)
        self.assertAlmostEqual(0.94509, scores['VectorExtremaCosineSimilarity'], places=5)
        self.assertAlmostEqual(0.960771, scores['GreedyMatchingScore'], places=5)
        self.assertEqual(18, len(scores))

        scores = n.compute_metrics(ref_list=[
            [
                "this is one reference sentence for sentence1",
                "this is a reference sentence for sentence2 which was generated by your model"
            ],
            [
                "this is one more reference sentence for sentence1",
                "this is the second reference sentence for sentence2"
            ],
        ],
            hyp_list=[
                "this is the model generated sentence1 which seems good enough",
                "this is sentence2 which has been generated by your model"
            ]
        )
        self.assertAlmostEqual(0.55, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(0.428174, scores['Bleu_2'], places=5)
        self.assertAlmostEqual(0.284043, scores['Bleu_3'], places=5)
        self.assertAlmostEqual(0.201143, scores['Bleu_4'], places=5)
        self.assertAlmostEqual(0.295797, scores['METEOR'], places=5)
        self.assertAlmostEqual(0.43973, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.22668, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.0606, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.03449, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.43973, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.30505, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.20132, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.23049, scores['Rouge_SU*'], places=5)
        self.assertAlmostEqual(1.242192, scores['CIDEr'], places=5)
        self.assertAlmostEqual(0.626149, scores['SkipThoughtCS'], places=5)
        self.assertAlmostEqual(0.88469, scores['EmbeddingAverageCosineSimilairty'], places=5)
        self.assertAlmostEqual(0.568696, scores['VectorExtremaCosineSimilarity'], places=5)
        self.assertAlmostEqual(0.784205, scores['GreedyMatchingScore'], places=5)
        self.assertEqual(18, len(scores))

        # Non-ASCII tests.
        scores = n.compute_individual_metrics(ref=["Test en français.",
                                                   "Le test en français."],
                                              hyp="Le test est en français.")
        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)
        self.assertAlmostEqual(0.0000051, scores['Bleu_3'], places=5)
        self.assertAlmostEqual(0, scores['Bleu_4'], places=5)
        self.assertAlmostEqual(0.48372379050300296, scores['METEOR'], places=5)
        self.assertAlmostEqual(0.85714, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.58824, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.30769, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.0, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.85714, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.74477, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.69565, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.73016, scores['Rouge_SU*'], places=5)
        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)
        self.assertAlmostEqual(0.9192341566085815, scores['SkipThoughtCS'], places=5)
        self.assertAlmostEqual(0.906562, scores['EmbeddingAverageCosineSimilairty'], places=5)
        self.assertAlmostEqual(0.815158, scores['VectorExtremaCosineSimilarity'], places=5)
        self.assertAlmostEqual(0.940959, scores['GreedyMatchingScore'], places=5)
        self.assertEqual(18, len(scores))

        n = NLGEval(metrics_to_omit=['ROUGE'])      # ROUGE does not work for Japanese, I don't know why...
        scores = n.compute_individual_metrics(ref=["テスト"],
                                              hyp="テスト")
        self.assertAlmostEqual(0.99999999, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(1.0, scores['METEOR'], places=3)
        self.assertAlmostEqual(1.0, scores['ROUGE_L'], places=3)
        self.assertAlmostEqual(0.0, scores['CIDEr'], places=3)
        self.assertAlmostEqual(1.0, scores['SkipThoughtCS'], places=3)
        self.assertAlmostEqual(1.0, scores['GreedyMatchingScore'], places=3)
        self.assertEqual(11, len(scores))

    def test_compute_metrics_omit(self):
        n = NLGEval(metrics_to_omit=['Bleu_3', 'METEOR', 'EmbeddingAverageCosineSimilairty'])

        # Individual Metrics
        scores = n.compute_individual_metrics(ref=["this is a test",
                                                   "this is also a test"],
                                              hyp="this is a good test")
        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)
        self.assertAlmostEqual(0.84211, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.4, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.18182, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.0, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.84211, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.68168, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.66667, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.70588, scores['Rouge_SU*'], places=5)
        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)
        self.assertAlmostEqual(0.8375251, scores['SkipThoughtCS'], places=5)
        self.assertAlmostEqual(0.94509, scores['VectorExtremaCosineSimilarity'], places=5)
        self.assertAlmostEqual(0.960771, scores['GreedyMatchingScore'], places=5)
        self.assertEqual(14, len(scores))

    def test_compute_metrics(self):
        # The example from the README.
        root_dir = os.path.join(os.path.dirname(__file__), '..', '..')
        hypothesis = os.path.join(root_dir, 'examples/hyp.txt')
        references = os.path.join(root_dir, 'examples/ref1.txt'), os.path.join(root_dir, 'examples/ref2.txt')
        scores = nlgeval.compute_metrics(hypothesis, references)
        self.assertAlmostEqual(0.55, scores['Bleu_1'], places=5)
        self.assertAlmostEqual(0.428174, scores['Bleu_2'], places=5)
        self.assertAlmostEqual(0.284043, scores['Bleu_3'], places=5)
        self.assertAlmostEqual(0.201143, scores['Bleu_4'], places=5)
        self.assertAlmostEqual(0.295797, scores['METEOR'], places=5)
        self.assertAlmostEqual(0.439730, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.226680, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.060600, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.034490, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.439730, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.305050, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.201320, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.230490, scores['Rouge_SU*'], places=5)
        self.assertAlmostEqual(1.242192, scores['CIDEr'], places=5)
        self.assertAlmostEqual(0.626149, scores['SkipThoughtCS'], places=5)
        self.assertAlmostEqual(0.88469, scores['EmbeddingAverageCosineSimilairty'], places=5)
        self.assertAlmostEqual(0.568696, scores['VectorExtremaCosineSimilarity'], places=5)
        self.assertAlmostEqual(0.784205, scores['GreedyMatchingScore'], places=5)
        self.assertEqual(11, len(scores))

    def test_rouge(self):
        n = NLGEval(metrics_to_omit=['Bleu_1', 'Bleu_2', 'Bleu_3', 'METEOR', 'EmbeddingAverageCosineSimilairty', 'CIDEr', 'SkipThoughtCS', 'VectorExtremaCosineSimilarity', 'GreedyMatchingScore'])

        # Individual Metrics
        scores = n.compute_individual_metrics(ref=["preprocess my summaries, then run ROUGE"],
                                              hyp="I only want to preprocess my summaries and then run ROUGE on my own")
        self.assertAlmostEqual(0.60000, scores['Rouge_1'], places=5)
        self.assertAlmostEqual(0.44444, scores['Rouge_2'], places=5)
        self.assertAlmostEqual(0.25000, scores['Rouge_3'], places=5)
        self.assertAlmostEqual(0.00000, scores['Rouge_4'], places=5)
        self.assertAlmostEqual(0.60000, scores['Rouge_L'], places=5)
        self.assertAlmostEqual(0.53131, scores['Rouge_W'], places=5)
        self.assertAlmostEqual(0.28303, scores['Rouge_S*'], places=5)
        self.assertAlmostEqual(0.32258, scores['Rouge_SU*'], places=5)
        self.assertEqual(8, len(scores))

if __name__ == "__main__":
     unittest.main()